{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea422ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import lxml.etree\n",
    "from PIL import Image\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# TODO 1 : Lorem Ipsum is just a random txt that devs use as a placeholder for multiple things (especially web developping) when you don't have the real text and just want to test your functionnality. Put a [Lorem Ipsum](https://www.lipsum.com/) of 3 paragraphs in a txt file using python, each paragraph delimited by two new line.\n",
    "\n",
    "with open('/Users/lyna/Desktop/M1/NoSQL/exemple1.txt', 'a+') as f:\n",
    "    f.write(\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam molestie placerat ligula in fermentum. Vestibulum dignissim efficitur risus eget porta. Aenean non risus in sem rutrum posuere quis id justo. Suspendisse enim purus, vehicula eget laoreet eget, iaculis quis enim. Cras magna risus, aliquet ut ligula ut, tincidunt finibus odio. Nunc ornare sollicitudin leo, ut aliquam mi scelerisque eu. Mauris metus ipsum, aliquam at elit ac, hendrerit cursus odio. Donec malesuada neque et justo imperdiet laoreet. Sed molestie sem lacus, sed finibus libero consectetur eget. Quisque pharetra, orci id convallis pretium, ipsum tortor scelerisque ipsum, a hendrerit leo ante sit amet mi. Phasellus eget placerat lorem, vel posuere dui. Suspendisse tempus enim ac ultricies gravida. Sed dui justo, dignissim eget dolor pellentesque, viverra sollicitudin mauris. Ut fermentum nunc ut odio placerat, a mattis mauris lobortis. Phasellus id ornare tellus. Cras vitae nibh felis.\" + \"\\n\\n\"\n",
    "           + \"Sed sit amet congue dui, at cursus ante. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse potenti. Mauris id diam augue. Etiam porttitor, urna a tincidunt placerat, purus mauris iaculis arcu, ut rhoncus est justo at dolor. Suspendisse sit amet augue sit amet arcu imperdiet commodo. Integer dignissim nec massa ac tempor. Mauris porttitor rhoncus nisl, vel pretium arcu dapibus sagittis. Nunc quis posuere enim, sit amet posuere neque. Aenean eget sagittis dui. Sed aliquam enim metus, vitae suscipit ex dapibus sollicitudin. Donec pellentesque cursus lobortis. Mauris viverra eget elit a malesuada. Nunc posuere commodo orci, sit amet mattis risus pharetra eu. Vestibulum rhoncus, enim eu fermentum accumsan, nibh augue laoreet sapien, eget elementum nulla arcu eget risus. Vivamus ut pharetra odio.\" + \"\\n\\n\"\n",
    "           + \"Phasellus maximus urna nunc, nec pellentesque nulla sodales et. Suspendisse id massa aliquam, commodo elit a, vestibulum neque. Cras a lacus hendrerit quam fringilla volutpat id nec felis. Vestibulum erat urna, posuere at mollis nec, convallis et arcu. Cras porta, magna sed facilisis ullamcorper, nisl dolor venenatis odio, non semper lacus ligula id nulla. Proin sed augue arcu. Nam nec odio condimentum, ornare purus ut, luctus tortor. Aliquam sed ullamcorper elit, non finibus metus. Maecenas nec dictum dui. Ut a mauris vel quam congue cursus.\" + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bbff9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2 : Update the txt file by removing the first paragraph.\n",
    "with open('/Users/lyna/Desktop/M1/NoSQL/exemple1.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "new_text = \"\\n\\n\".join(text.split(\"\\n\\n\")[1:])\n",
    "\n",
    "with open('/Users/lyna/Desktop/M1/NoSQL/exemple1.txt', 'w') as f:\n",
    "    f.write(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b2f8c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LeCun et al.': {'title': 'Deep Learning', 'author_and_affiliations': {'Yann LeCun': ['Facebook AI Research', 'New York University'], 'Yoshua Bengio': ['Department of Computer Science and Operations Research Université de Montréal'], 'Geoffrey Hinton': ['Google', 'Department of Computer Science, University of Toronto']}}, 'Goodfellow et al.': {'title': 'Generative Adversarial Nets', 'author_and_affiliations': {'Ian Goodfellow': ['Universite de Montreal'], 'Jean Pouget-Abadie': ['Ecole Polytechnique'], 'Mehdi Mirza': ['Université de Montréal'], 'Bing Xu': ['Université de Montréal'], 'David Warde-Farley': ['Université de Montréal'], 'Sherjil Ozair': ['Indian Institute of Technology Delhi'], 'Aaron Courville': ['Université de Montréal'], 'Yoshua Bengio': ['CIFAR Senior Fellow']}}}\n"
     ]
    }
   ],
   "source": [
    "# TODO 3 : Create a dict from the paper of [lecun et al.] and [goodfellow et al.] with authors, title, affiliations.\n",
    "lecun_paper = {\n",
    "    \"title\": \"Deep Learning\",\n",
    "    \"author_and_affiliations\": {\n",
    "        \"Yann LeCun\": [\"Facebook AI Research\", \"New York University\"],\n",
    "        \"Yoshua Bengio\": [\"Department of Computer Science and Operations Research Université de Montréal\"],\n",
    "        \"Geoffrey Hinton\": [\"Google\", \"Department of Computer Science, University of Toronto\"]\n",
    "    }\n",
    "}\n",
    "goodfellow_paper = {\n",
    "    \"title\": \"Generative Adversarial Nets\",\n",
    "    \"author_and_affiliations\": {\n",
    "        \"Ian Goodfellow\": [\"Universite de Montreal\"],\n",
    "        \"Jean Pouget-Abadie\": [\"Ecole Polytechnique\"],\n",
    "        \"Mehdi Mirza\": [\"Université de Montréal\"],\n",
    "        \"Bing Xu\": [\"Université de Montréal\"],\n",
    "        \"David Warde-Farley\": [\"Université de Montréal\"],\n",
    "        \"Sherjil Ozair\": [\"Indian Institute of Technology Delhi\"],\n",
    "        \"Aaron Courville\": [\"Université de Montréal\"],\n",
    "        \"Yoshua Bengio\": [\"CIFAR Senior Fellow\"]\n",
    "    }\n",
    "}\n",
    "papers_dict = {\n",
    "    \"LeCun et al.\": lecun_paper,\n",
    "    \"Goodfellow et al.\": goodfellow_paper\n",
    "}\n",
    "print(papers_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3985bca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LeCun et al.': {'title': 'Deep Learning', 'author_and_affiliations': {'Yann LeCun': ['Facebook AI Research', 'New York University'], 'Yoshua Bengio': ['Department of Computer Science and Operations Research Université de Montréal'], 'Geoffrey Hinton': ['Google', 'Department of Computer Science, University of Toronto']}}, 'Goodfellow et al.': {'title': 'Generative Adversarial Nets', 'author_and_affiliations': {'Ian Goodfellow': ['Universite de Montreal'], 'Jean Pouget-Abadie': ['Ecole Polytechnique'], 'Mehdi Mirza': ['Université de Montréal'], 'Bing Xu': ['Université de Montréal'], 'David Warde-Farley': ['Université de Montréal'], 'Sherjil Ozair': ['Indian Institute of Technology Delhi'], 'Aaron Courville': ['Université de Montréal'], 'Yoshua Bengio': ['CIFAR Senior Fellow']}}}\n"
     ]
    }
   ],
   "source": [
    "# TODO 4 : Save the previously created dict in the JSON format and load it back.\n",
    "\n",
    "with open('papers_dict.json', 'w') as fp:\n",
    "    json.dump(papers_dict, fp)\n",
    "\n",
    "with open('papers_dict.json', 'r') as fp:\n",
    "    loaded_dict = json.load(fp)\n",
    "    \n",
    "print(loaded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84c9cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5 : Save the previously created dict in the pickle format. Try to open manually (i.e with a text editor), is it human readable ?\n",
    "\n",
    "with open('/Users/lyna/Desktop/M1/NoSQL/papers_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(papers_dict, f)\n",
    "    \n",
    "# On ne voit qu'une suite de nombres composés de quatre chiffres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa3dbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6 : Parse the xml_file2 in the same way as seen in the lecture: put infos in a dict and save it in a json file.\n",
    "\n",
    "# charge la data \n",
    "xml_file2 = \"/Users/lyna/Documents/GitHub/NoSQL/data/Chap2/xml_file2.nxml\"\n",
    "root = lxml.etree.parse(xml_file2)\n",
    "\n",
    "# stocke les différentes infos\n",
    "date = root.xpath(\"//date//text()\")\n",
    "hour = root.xpath(\"//hour//text()\")\n",
    "to = root.xpath(\"//to//text()\")\n",
    "from_ = root.xpath(\"//from//text()\")\n",
    "body = root.xpath(\"//body//text()\")\n",
    "\n",
    "# Crée un dictionnaire avec les informations\n",
    "info_dict = {\n",
    "    \"date\": date,\n",
    "    \"hour\": hour,\n",
    "    \"to\": to,\n",
    "    \"from\": from_,\n",
    "    \"body\": body\n",
    "}\n",
    "\n",
    "# Enregistre le dictionnaire dans un fichier JSON\n",
    "with open(\"info.json\", \"w\") as outfile:\n",
    "    json.dump(info_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e69a283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7 : Download an image of your choice and save it in either jpg or png.\n",
    "\n",
    "im = Image.open(requests.get(\"https://cdn.wamiz.fr/cdn-cgi/image/format=auto,quality=80,width=776,fit=contain/article/images/15876197_1368431473208290_144086124032163840_n(1)-9798.jpg\", stream=True).raw)\n",
    "im.save(\"/Users/lyna/Desktop/M1/NoSQL/chat_mimi.jpg\", \"jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343601da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 8 : From the data/Chap2/data_world.json file, create a set of publisher type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f03e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 9 : From the data/Chap2/data_world.json file, delete the key of your choice and save the new dict as data_world_cleaned.json.\n",
    "with open('/Users/lyna/Documents/GitHub/NoSQL/data/Chap2/data_world.json', 'r', encoding='utf-8') as fp:\n",
    "    docs = json.load(fp)\n",
    "\n",
    "[doc.pop(\"accessLevel\") for doc in docs if doc[\"publisher\"][\"@type\"] == \"org:Organization\"]\n",
    "\n",
    "with open('/Users/lyna/Desktop/M1/NoSQL/data_world_cleaned.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(docs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85d0922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accrualPeriodicity  R/P1D  R/P1M  R/P3M  R/PT1S  irregular\n",
      "accessLevel                                               \n",
      "public                  5      3      1       1       4961\n"
     ]
    }
   ],
   "source": [
    "# TODO 10 bonus : From the data/Chap2/data_world.json file, create the co-occurence matrix between \"accessLevel\" and \"accrualPeriodicity\".\n",
    "\n",
    "with open('/Users/lyna/Documents/GitHub/NoSQL/data/Chap2/data_world.json', 'r', encoding='utf-8') as fp:\n",
    "    docs = json.load(fp)\n",
    "\n",
    "# Création d'un dataframe de la data\n",
    "df = pd.DataFrame(docs)\n",
    "\n",
    "# Matrice de co occurence\n",
    "matrix = pd.crosstab(df['accessLevel'], df['accrualPeriodicity'])\n",
    "print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
